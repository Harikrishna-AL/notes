<!DOCTYPE html>






























<html
  class="not-ready text-sm lg:text-base"
  style="--bg: #faf6f1"
  lang="en-us"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>NeRF - Notes</title>

  
  <meta name="theme-color" />

  
  
  
  <meta name="description" content="what are NeRFs? NeRFs are a type of network used to generate unseen views of an object using the seen views as the dataset. The model takes a 5D(3d coordinates,camera angles) input and then outputs a 4D(rgb color, densoty) vector.
Sampling Inputs The 3 dimensinal coordinates that has to be passed into the neural network is sampled using a ray casting algorithm. The algorithm is as follows:
Cast a ray from a pixel of the image to the object." />
  <meta name="author" content="Notes" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://harikrishna-al.github.io/notes/main.min.css" />

  
  <script
    defer
    src="https://harikrishna-al.github.io/notes/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
   
  <link rel="preload" as="image" href="https://harikrishna-al.github.io/notes/theme.png" />

  
  
  
  <link rel="preload" as="image" href="https://avatars.githubusercontent.com/u/91690484?v=4" />
  
  

  

  
  <link rel="icon" href="https://harikrishna-al.github.io/notes/favicon.ico" />
  <link rel="apple-touch-icon" href="https://harikrishna-al.github.io/notes/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.111.3">

  
  

  
  
  
  
  
  
  
  <meta property="og:title" content="NeRF" />
<meta property="og:description" content="what are NeRFs? NeRFs are a type of network used to generate unseen views of an object using the seen views as the dataset. The model takes a 5D(3d coordinates,camera angles) input and then outputs a 4D(rgb color, densoty) vector.
Sampling Inputs The 3 dimensinal coordinates that has to be passed into the neural network is sampled using a ray casting algorithm. The algorithm is as follows:
Cast a ray from a pixel of the image to the object." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harikrishna-al.github.io/notes/posts/nerf_model/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-23T21:57:32+05:30" />
<meta property="article:modified_time" content="2023-04-23T21:57:32+05:30" />

  
  <meta itemprop="name" content="NeRF">
<meta itemprop="description" content="what are NeRFs? NeRFs are a type of network used to generate unseen views of an object using the seen views as the dataset. The model takes a 5D(3d coordinates,camera angles) input and then outputs a 4D(rgb color, densoty) vector.
Sampling Inputs The 3 dimensinal coordinates that has to be passed into the neural network is sampled using a ray casting algorithm. The algorithm is as follows:
Cast a ray from a pixel of the image to the object."><meta itemprop="datePublished" content="2023-04-23T21:57:32+05:30" />
<meta itemprop="dateModified" content="2023-04-23T21:57:32+05:30" />
<meta itemprop="wordCount" content="237">
<meta itemprop="keywords" content="" />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="NeRF"/>
<meta name="twitter:description" content="what are NeRFs? NeRFs are a type of network used to generate unseen views of an object using the seen views as the dataset. The model takes a 5D(3d coordinates,camera angles) input and then outputs a 4D(rgb color, densoty) vector.
Sampling Inputs The 3 dimensinal coordinates that has to be passed into the neural network is sampled using a ray casting algorithm. The algorithm is as follows:
Cast a ray from a pixel of the image to the object."/>

  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-0.5 text-3xl font-bold"
      href="https://harikrishna-al.github.io/"
      >Notes</a
    >
    <div
      class="btn-dark text-[0] ml-6 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  
  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = `"#faf6f1"`.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    

    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-10rem)] max-w-3xl px-8 pt-16 pb-24 dark:prose-invert"
    >
      

<article>
  <header class="mb-20">
    <h1 class="!my-0 pb-2.5">NeRF</h1>

    
    <div class="text-sm opacity-60">
      
      <time>Apr 23, 2023</time>
      
      
      
      
    </div>
    
  </header>

  <section><ul>
<li>
<h2 id="_what-are-nerfs_"><em><strong>what are NeRFs?</strong></em></h2>
<p>NeRFs are a type of network used to generate unseen views of an object using the seen views as the dataset. The model takes a 5D(3d coordinates,camera angles) input and then outputs a 4D(rgb color, densoty) vector.</p>
<!-- raw HTML omitted -->
</li>
<li>
<h2 id="_sampling-inputs_"><em><strong>Sampling Inputs</strong></em></h2>
<p>The 3 dimensinal coordinates that has to be passed into the neural network is sampled using a ray casting algorithm. The algorithm is as follows:</p>
<ol>
<li>Cast a ray from a pixel of the image to the object.</li>
<li>Sample points along the ray.</li>
<li>Pass the sampled points to the network.</li>
</ol>
<p><img src="ray_cast.png" alt="Image alt"></p>
<p>Code:</p>
<pre tabindex="0"><code>def compute_rays(height,width,focal_length,cam2world):
    x ,y = tor_to_meshgrid(
        torch.arrange(width).to(cam2world),
        torch.arrange(height).to(cam2world)
    )
    directions = torch.stack(
        [(x - width*0.5)/focal_length,
        (y - height*0.5)/focal_length,
        torch.ones_like(x)]
    )
    ray_o = torch.broadcast_to(
        cam2world[:3,-1],
        directions.shape
    )
    ray_dir = torch.sum(
        directions[...,None,:] * cam2world[:3,:3],
        dim=-1
    )
    return rays_o,ray_dir   
</code></pre><p>This function returns all the points on the image and it&rsquo;s direction vector. The points are then sampled using the following function:</p>
<pre tabindex="0"><code>def compute_query_points(ray_directions,ray_origins,near far,num_samples,random=True):
    depth_values = torch.linspace(near,far,num_samples).to(ray_origins)
    if random:
        shape = list(depth_values.shpae[:,-1]) + [num_samples]
        depth_values = depth_values + torch.rand(shape).to(ray_origins) * (far - near)/num_samples
    query_points = ray_origins[...,None,:] + ray_directions[...,None,:]*depth_values[...,None,:]
    return query_points, depth_values 
</code></pre></li>
<li>
<h2 id="_archeitecture_"><em><strong>Archeitecture</strong></em></h2>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><img src="nerf.jpeg" alt="Image alt"></p>
<!-- raw HTML omitted -->
<p>The network is a fully connected network with 8 layers. The input to the network is a 5D vector and the output is a 4D vector. The network is trained using a loss function which is the sum of the MSE of the rgb color and the density. The network is trained using the Adam optimizer.</p>
</li>
<li>
<h2 id="heading"></h2>
</li>
</ul>
</section>

  
  

  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    <a
      class="flex w-1/2 items-center rounded-l-md p-6 pr-3 no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://harikrishna-al.github.io/notes/posts/nerf/"
      ><span class="mr-1.5">←</span><span>NeRF (Neural Radience Fields)</span></a
    >
    
    
    <a
      class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://harikrishna-al.github.io/notes/posts/diffusion/"
      ><span>Diffusion</span><span class="ml-1.5">→</span></a
    >
    
  </nav>
  

  
  

  
  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2023
    <a class="link" href="https://harikrishna-al.github.io/notes/">Notes</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >Powered by Hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >Theme Paper</a
  >
</footer>

  </body>
</html>
